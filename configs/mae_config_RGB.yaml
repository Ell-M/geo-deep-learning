seed_everything: true

trainer:
  accelerator: "gpu"
  devices: -1
  strategy:
    class_path: lightning.pytorch.strategies.DDPStrategy
    init_args:
      find_unused_parameters: false
      gradient_as_bucket_view: true
      static_graph: true
  gradient_clip_val: 1.0
  precision: "16-mixed"
  sync_batchnorm: true
  logger:
    class_path: lightning.pytorch.loggers.mlflow.MLFlowLogger
    init_args:
      save_dir: ./logs
      log_model: false
      experiment_name: "gdl_experiment"
      run_name: "mae_rgb"
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: "train_loss"
        mode: "min"
        save_top_k: 3
        filename: "model_e{epoch:02d}_tloss{train_loss:.3f}"
  max_epochs: 100

model:
  class_path: tasks_with_models.pretrain_mae.PretrainMAE
  init_args:
    image_size: [512, 512]
    in_channels: 3
    patch_size: 16
    mask_ratio: 0.75
    embed_dim: 768
    depth: 12
    num_heads: 12
    decoder_embed_dim: 512
    decoder_depth: 8
    decoder_num_heads: 16
    mlp_ratio: 4.0
    norm_pix_loss: false
    max_samples: 10
    optimizer:
      class_path: torch.optim.AdamW
      init_args:
        lr: 1e-3
        weight_decay: 0.05
        betas: [0.9, 0.95]
    scheduler:
      class_path: geo_deep_learning.tools.schedulers.lr_scheduler.MaeLRSchedulerFactory
      init_args:
        accum_iter: 4
        min_lr: 0
        warmup_epochs: 4
        iteration_per_epoch: null
        max_iterations: null
    scheduler_config:
      interval: "step"
      frequency: 1
    weights_from_checkpoint_path: null

data:
  class_path: datamodules.pretrain_datamodule.PretrainDataModule
  init_args:
    batch_size: 16
    num_workers: 8
    data_type_max: 255
    patch_size:
    - 512
    - 512
    mean: null
    std: null
    csv_root_folder: ./data/pretrain
    patches_root_folder: /gpfs/fs5/nrcan/nrcan_geobase/work/data/data_mae/jamaica_subset/pretrain_dataset/tiles

ckpt_path: null #for resuming
